{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1hHVznqfCycdLJeC0ZqBzcps92gXTjpga",
      "authorship_tag": "ABX9TyMawYOLwL5QxINiws9e5cf0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Token-05/FACERecognition/blob/main/facerecognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHmJZAmC55nE",
        "outputId": "c5da0a34-2c85-4708-f005-ef1fe8317224"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U keras==2.9.0 \n",
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRY6mLwu6Y8Q",
        "outputId": "c44d2bf9-c32d-408d-b9eb-6f2c7ef98a54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras==2.9.0 in /usr/local/lib/python3.8/dist-packages (2.9.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (2.9.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.1.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.3.0)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.28.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow) (21.3)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.51.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (14.0.6)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (4.4.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.15.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (5.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow) (3.0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiaerEg33DQD"
      },
      "outputs": [],
      "source": [
        "# 入力処理系\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "# ネットワーク系\n",
        "from keras import models\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, Lambda, ELU, concatenate, GlobalAveragePooling2D, Input, BatchNormalization, SeparableConv2D, Subtract, concatenate, Conv2D, AveragePooling2D\n",
        "from keras.activations import relu, softmax\n",
        "from keras.layers.convolutional import Convolution2D\n",
        "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
        "from keras.optimizers import Adam, RMSprop, SGD\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 1.0 (～中間発表)\n",
        "# network : squeeze net\n",
        "class faceid_beta:\n",
        "    # 初期化\n",
        "    def __init__(self, x):\n",
        "        self.x = x\n",
        "    \n",
        "    # 同一人物の画像カップル\n",
        "    def create_couple_rgbd(self):\n",
        "        # ファイルパス\n",
        "        under_vap_path, under_cap_path = random.randint(1, 51), random.randint(1, 3)\n",
        "        # rgb(sync)\n",
        "        im1 = np.array(Image.open('/content/drive/MyDrive/ColabNotebooks/graduation_research/main/VAP_trimodal_reduced/{}/Cap/{}/SyncRGB/1.jpg'.format(under_vap_path,under_cap_path)))[140:340,220:420]\n",
        "        im2 = np.array(Image.open('/content/drive/MyDrive/ColabNotebooks/graduation_research/main/VAP_trimodal_reduced/{}/Cap/{}/SyncRGB/2.jpg'.format(under_vap_path,under_cap_path)))[140:340,220:420]\n",
        "        # d(sync)\n",
        "        imd1 = np.array(Image.open('/content/drive/MyDrive/ColabNotebooks/graduation_research/main/VAP_trimodal_reduced/{}/Cap/{}/SyncD/1.png'.format(under_vap_path,under_cap_path)))[140:340,220:420]\n",
        "        imd2 = np.array(Image.open('/content/drive/MyDrive/ColabNotebooks/graduation_research/main/VAP_trimodal_reduced/{}/Cap/{}/SyncD/2.png'.format(under_vap_path,under_cap_path)))[140:340,220:420]\n",
        "        # 正規化\n",
        "        im1_truth = (imd1-np.mean(imd1))/np.max(imd1)\n",
        "        im2_truth = (imd2-np.mean(imd2))/np.max(imd2)\n",
        "        # 元の画像がRGBなのでRGBDに作り替える\n",
        "        full1 = np.zeros((200,200,4))\n",
        "        full1[:,:,:3] = im1[:,:,:3]\n",
        "        full1[:,:,3] = im1_truth\n",
        "        full2 = np.zeros((200,200,4))\n",
        "        full2[:,:,:3] = im2[:,:,:3]\n",
        "        full2[:,:,3] = im2_truth\n",
        "        return np.array([full1, full2])\n",
        "    \n",
        "    # 同一人物でない画像カップル\n",
        "    def create_wrong_rgbd(self):\n",
        "        # ファイルパス\n",
        "        under_vap_path = random.sample(list(range(1,52)),2)\n",
        "        under_cap_path = random.sample(list(range(1,4)),2)\n",
        "        # rgb(sync)\n",
        "        im1 = np.array(Image.open('/content/drive/MyDrive/ColabNotebooks/graduation_research/main/VAP_trimodal_reduced/{}/Cap/{}/SyncRGB/1.jpg'.format(under_vap_path[0],under_cap_path[0])))[140:340,220:420]\n",
        "        im2 = np.array(Image.open('/content/drive/MyDrive/ColabNotebooks/graduation_research/main/VAP_trimodal_reduced/{}/Cap/{}/SyncRGB/2.jpg'.format(under_vap_path[1],under_cap_path[1])))[140:340,220:420]\n",
        "        # d(sync)\n",
        "        imd1 = np.array(Image.open('/content/drive/MyDrive/ColabNotebooks/graduation_research/main/VAP_trimodal_reduced/{}/Cap/{}/SyncD/1.png'.format(under_vap_path[0],under_cap_path[0])))[140:340,220:420]\n",
        "        imd2 = np.array(Image.open('/content/drive/MyDrive/ColabNotebooks/graduation_research/main/VAP_trimodal_reduced/{}/Cap/{}/SyncD/2.png'.format(under_vap_path[0],under_cap_path[1])))[140:340,220:420]\n",
        "        # 正規化\n",
        "        im1_truth = (imd1-np.mean(imd1))/np.max(imd1)\n",
        "        im2_truth = (imd2-np.mean(imd2))/np.max(imd2)\n",
        "        # 元の画像がRGBなのでRGBDに作り替える\n",
        "        full1 = np.zeros((200,200,4))\n",
        "        full1[:,:,:3] = im1[:,:,:3]\n",
        "        full1[:,:,3] = im1_truth\n",
        "        full2 = np.zeros((200,200,4))\n",
        "        full2[:,:,:3] = im2[:,:,:3]\n",
        "        full2[:,:,3] = im2_truth\n",
        "        return np.array([full1, full2])\n",
        "    \n",
        "    # ユークリッド距離の導出(顔の類似度チェック)\n",
        "    def euclidean_distance(self, inputs):\n",
        "        assert len(inputs) == 2, 'Euclidean distance needs 2 inputs, %d given' % len(inputs)\n",
        "        u, v = inputs\n",
        "        return K.sqrt(K.sum((K.square(u - v)), axis=1, keepdims=True))\n",
        "    \n",
        "    # 対となるサンプル同士の近傍・分離\n",
        "    def contrastive_loss(self,y_true,y_pred):\n",
        "        margin=1.0\n",
        "        return K.mean((margin - y_true) * K.square(y_pred) + y_true * K.square(K.maximum(margin - y_pred, 0.0)))\n",
        "        # return K.mean( K.square(y_pred) )\n",
        "    \n",
        "    # inception module (https://pystyle.info/pytorch-googlenet/#outline__2_2)\n",
        "    def fire(self, x, squeeze=16, expand=64):\n",
        "        x = Conv2D(squeeze, (1,1), padding='valid', activation='relu')(x)\n",
        "        # 1x1だと端のデータも特徴量として算出しやすい\n",
        "        left = Conv2D(expand, (1,1), padding='valid', activation='relu')(x)\n",
        "        # 3x3だと厳しいのでゼロパディングして端のデータに対して調整\n",
        "        right = Conv2D(expand, (3,3), padding='same', activation='relu')(x)\n",
        "        # 合体\n",
        "        x = concatenate([left, right], axis=3)\n",
        "        return x\n",
        "    \n",
        "    # 畳み込み\n",
        "    def convolution(self):\n",
        "      img_input=Input(shape=(200,200,4))\n",
        "      x = Conv2D(64, (5, 5), strides=(2, 2), padding='valid')(img_input)\n",
        "      x = BatchNormalization()(x)\n",
        "      x = Activation('relu')(x)\n",
        "      x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
        "      x = self.fire(x, squeeze=16, expand=16)\n",
        "      x = self.fire(x, squeeze=16, expand=16)\n",
        "      x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
        "      x = self.fire(x, squeeze=32, expand=32)\n",
        "      x = self.fire(x, squeeze=32, expand=32)\n",
        "      x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
        "      x = self.fire(x, squeeze=48, expand=48)\n",
        "      x = self.fire(x, squeeze=48, expand=48)\n",
        "      x = self.fire(x, squeeze=64, expand=64)\n",
        "      x = self.fire(x, squeeze=64, expand=64)\n",
        "      x = Dropout(0.2)(x)\n",
        "      x = Conv2D(512, (1, 1), padding='same')(x)\n",
        "      out = Activation('relu')(x)\n",
        "      modelsqueeze= Model(img_input, out)\n",
        "      modelsqueeze.summary()\n",
        "      im_in = Input(shape=(200,200,4))\n",
        "      x1 = modelsqueeze(im_in)\n",
        "      x1 = Flatten()(x1)\n",
        "      x1 = Dense(512, activation=\"relu\")(x1)\n",
        "      x1 = Dropout(0.2)(x1)\n",
        "      x1 = BatchNormalization()(x1)\n",
        "      feat_x = Dense(128, activation=\"linear\")(x1)\n",
        "      feat_x = Lambda(lambda  x: K.l2_normalize(x,axis=1))(feat_x)\n",
        "      model_top = Model(inputs = [im_in], outputs = feat_x)\n",
        "      model_top.summary()\n",
        "      im_in1 = Input(shape=(200,200,4))\n",
        "      im_in2 = Input(shape=(200,200,4))\n",
        "      feat_x1 = model_top(im_in1)\n",
        "      feat_x2 = model_top(im_in2)\n",
        "      lambda_merge = Lambda(self.euclidean_distance)([feat_x1, feat_x2])\n",
        "      model_final = Model(inputs = [im_in1, im_in2], outputs = lambda_merge)\n",
        "      model_final.summary()\n",
        "      adam = Adam(learning_rate=0.001)\n",
        "      model_final.compile(optimizer=adam, loss=self.contrastive_loss, metrics=['accuracy'])\n",
        "      return model_final\n",
        "    \n",
        "    def show_graph(self):\n",
        "      gen = self.generator(32)\n",
        "      val_gen = self.val_generator(8)\n",
        "      model_final = self.convolution()\n",
        "      history = model_final.fit(gen, steps_per_epoch=30, epochs=50, validation_data = val_gen, validation_steps=30)\n",
        "      cop = self.create_wrong_rgbd()\n",
        "      outputs = model_final.predict([cop[0].reshape((1,200,200,4)), cop[1].reshape((1,200,200,4))])\n",
        "      print(outputs)\n",
        "\n",
        "      plt.plot(history.history['accuracy'])\n",
        "      plt.plot(history.history['val_accuracy'])\n",
        "      plt.title('Model accuracy')\n",
        "      plt.ylabel('Accuracy')\n",
        "      plt.xlabel('Epoch')\n",
        "      plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "      plt.show()\n",
        "\n",
        "      # Plot training & validation loss values\n",
        "      plt.plot(history.history['loss'])\n",
        "      plt.plot(history.history['val_loss'])\n",
        "      plt.title('Model loss')\n",
        "      plt.ylabel('Loss')\n",
        "      plt.xlabel('Epoch')\n",
        "      plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "      plt.show()\n",
        "    \n",
        "    # train data set (訓練データ)\n",
        "    def generator(self, batch_size):\n",
        "        while 1:\n",
        "            X=[]\n",
        "            y=[]\n",
        "            switch=True\n",
        "            for _ in range(batch_size):\n",
        "                if switch:\n",
        "                #   print(\"correct\")\n",
        "                    X.append(self.create_couple_rgbd().reshape((2,200,200,4)))\n",
        "                    y.append(np.array([0.]))\n",
        "                else:\n",
        "                #   print(\"wrong\")\n",
        "                    X.append(self.create_wrong_rgbd().reshape((2,200,200,4)))\n",
        "                    y.append(np.array([1.]))\n",
        "                switch=not switch\n",
        "            X = np.asarray(X)\n",
        "            y = np.asarray(y)\n",
        "            XX1=X[0,:]\n",
        "            XX2=X[1,:]\n",
        "            yield [X[:,0],X[:,1]],y\n",
        "    \n",
        "    # val data set (検証用データ)\n",
        "    def val_generator(self, batch_size):\n",
        "        while 1:\n",
        "            X=[]\n",
        "            y=[]\n",
        "            switch=True\n",
        "            for _ in range(batch_size):\n",
        "                if switch:\n",
        "                    X.append(self.create_couple_rgbd().reshape((2,200,200,4)))\n",
        "                    y.append(np.array([0.]))\n",
        "                else:\n",
        "                    X.append(self.create_wrong_rgbd().reshape((2,200,200,4)))\n",
        "                    y.append(np.array([1.]))\n",
        "                switch=not switch\n",
        "            X = np.asarray(X)\n",
        "            y = np.asarray(y)\n",
        "            XX1=X[0,:]\n",
        "            XX2=X[1,:]\n",
        "            yield [X[:,0],X[:,1]],y"
      ],
      "metadata": {
        "id": "zrwU2ipM5yKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# version 1.1 (～年末)\n",
        "# network : resnet\n",
        "class faceid_v1_1:\n",
        "  # 初期化\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  \n",
        "  # 同一人物の画像カップル\n",
        "  def create_couple_rgbd(self, folder_name):\n",
        "    # ファイルパスの設定\n",
        "    if folder_name == 'train':\n",
        "      under_vap_path, under_cap_path, under_cap_path2 = random.randint(1, 40), random.randint(1, 100), random.randint(1, 100)\n",
        "    elif folder_name == 'val':\n",
        "      under_vap_path, under_cap_path, under_cap_path2 = random.randint(41, 51), random.randint(1, 100), random.randint(1, 100)\n",
        "    # rgb(sync)\n",
        "    im1 = np.array(Image.open('/content/drive/MyDrive/ColabNotebooks/graduation_research/main/VAP_trimodal_reduced/{}/Cap/3/SyncRGB/{}.jpg'.format(under_vap_path,under_cap_path)))[140:340,220:420]\n",
        "    im2 = np.array(Image.open('/content/drive/MyDrive/ColabNotebooks/graduation_research/main/VAP_trimodal_reduced/{}/Cap/3/SyncRGB/{}.jpg'.format(under_vap_path,under_cap_path2)))[140:340,220:420]\n",
        "    # d(sync)\n",
        "    imd1 = np.array(Image.open('/content/drive/MyDrive/ColabNotebooks/graduation_research/main/VAP_trimodal_reduced/{}/Cap/3/SyncD/{}.png'.format(under_vap_path,under_cap_path)))[140:340,220:420]\n",
        "    imd2 = np.array(Image.open('/content/drive/MyDrive/ColabNotebooks/graduation_research/main/VAP_trimodal_reduced/{}/Cap/3/SyncD/{}.png'.format(under_vap_path,under_cap_path2)))[140:340,220:420]\n",
        "    # 正規化\n",
        "    im1_truth = (imd1-np.mean(imd1))/np.max(imd1)\n",
        "    im2_truth = (imd2-np.mean(imd2))/np.max(imd2)\n",
        "    # 元の画像がRGBなのでRGBDに作り替える\n",
        "    full1 = np.zeros((200,200,4))\n",
        "    full1[:,:,:3] = im1[:,:,:3]\n",
        "    full1[:,:,3] = im1_truth\n",
        "    full2 = np.zeros((200,200,4))\n",
        "    full2[:,:,:3] = im2[:,:,:3]\n",
        "    full2[:,:,3] = im2_truth\n",
        "    return np.array([full1, full2])\n",
        "  \n",
        "  # 同一人物でない画像カップル\n",
        "  def create_wrong_rgbd(self, folder_name):\n",
        "    # ファイルパスの設定\n",
        "    if folder_name == 'train':\n",
        "      under_vap_path, under_cap_path = random.sample(list(range(1,41)),2), random.sample(list(range(1,101)),2)\n",
        "    elif folder_name == 'val':\n",
        "      under_vap_path, under_cap_path = random.sample(list(range(41,52)),2), random.sample(list(range(1,101)),2)\n",
        "    # rgb(sync)\n",
        "    im1 = np.array(Image.open('/content/drive/MyDrive/ColabNotebooks/graduation_research/main/VAP_trimodal_reduced/{}/Cap/3/SyncRGB/{}.jpg'.format(under_vap_path[0],under_cap_path[0])))[140:340,220:420]\n",
        "    im2 = np.array(Image.open('/content/drive/MyDrive/ColabNotebooks/graduation_research/main/VAP_trimodal_reduced/{}/Cap/3/SyncRGB/{}.jpg'.format(under_vap_path[1],under_cap_path[1])))[140:340,220:420]\n",
        "    # d(sync)\n",
        "    imd1 = np.array(Image.open('/content/drive/MyDrive/ColabNotebooks/graduation_research/main/VAP_trimodal_reduced/{}/Cap/3/SyncD/{}.png'.format(under_vap_path[0],under_cap_path[0])))[140:340,220:420]\n",
        "    imd2 = np.array(Image.open('/content/drive/MyDrive/ColabNotebooks/graduation_research/main/VAP_trimodal_reduced/{}/Cap/3/SyncD/{}.png'.format(under_vap_path[1],under_cap_path[1])))[140:340,220:420]\n",
        "    # 正規化\n",
        "    im1_truth = (imd1-np.mean(imd1))/np.max(imd1)\n",
        "    im2_truth = (imd2-np.mean(imd2))/np.max(imd2)\n",
        "    # 元の画像がRGBなのでRGBDに作り替える\n",
        "    full1 = np.zeros((200,200,4))\n",
        "    full1[:,:,:3] = im1[:,:,:3]\n",
        "    full1[:,:,3] = im1_truth\n",
        "    full2 = np.zeros((200,200,4))\n",
        "    full2[:,:,:3] = im2[:,:,:3]\n",
        "    full2[:,:,3] = im2_truth\n",
        "    return np.array([full1, full2])\n",
        "  \n",
        "  # ユークリッド距離の導出(顔の類似度チェック)\n",
        "  def euclidean_distance(self, inputs):\n",
        "    assert len(inputs) == 2, 'Euclidean distance needs 2 inputs, %d given' % len(inputs)\n",
        "    u, v = inputs\n",
        "    return K.sqrt(K.sum((K.square(u - v)), axis=1, keepdims=True))\n",
        "  \n",
        "  # 対となるサンプル同士の近傍・分離\n",
        "  def contrastive_loss(self,y_true,y_pred):\n",
        "    margin=1.0\n",
        "    return K.mean((margin - y_true) * K.square(y_pred) + y_true * K.square(K.maximum(margin - y_pred, 0.0)))\n",
        "  \n",
        "  # 訓練データのミニバッチ作成\n",
        "  def generator(self, batch_size):\n",
        "    while 1:\n",
        "      X=[]\n",
        "      y=[]\n",
        "      switch=True\n",
        "      for _ in range(batch_size):\n",
        "        if switch:\n",
        "          X.append(self.create_couple_rgbd('train').reshape((2,200,200,4)))\n",
        "          y.append(np.array([0.]))\n",
        "        else:\n",
        "          X.append(self.create_wrong_rgbd('train').reshape((2,200,200,4)))\n",
        "          y.append(np.array([1.]))\n",
        "        switch=not switch\n",
        "      X = np.asarray(X)\n",
        "      y = np.asarray(y)\n",
        "      XX1=X[0,:]\n",
        "      XX2=X[1,:]\n",
        "      yield [X[:,0],X[:,1]],y\n",
        "  \n",
        "  # 検証用データのミニバッチ作成\n",
        "  def val_generator(self, batch_size):\n",
        "    while 1:\n",
        "      X=[]\n",
        "      y=[]\n",
        "      switch=True\n",
        "      for _ in range(batch_size):\n",
        "        if switch:\n",
        "          X.append(self.create_couple_rgbd('val').reshape((2,200,200,4)))\n",
        "          y.append(np.array([0.]))\n",
        "        else:\n",
        "          X.append(self.create_wrong_rgbd('val').reshape((2,200,200,4)))\n",
        "          y.append(np.array([1.]))\n",
        "        switch=not switch\n",
        "      X = np.asarray(X)\n",
        "      y = np.asarray(y)\n",
        "      XX1=X[0,:]\n",
        "      XX2=X[1,:]\n",
        "      yield [X[:,0],X[:,1]],y\n",
        "  \n",
        "  # 基礎の畳み込み(ミクロ)\n",
        "  def basic_conv2d(self, x, filters, kernel_size, strides, padding):\n",
        "    x = Conv2D(filters, kernel_size, strides, padding)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "  \n",
        "  # InceptionModule\n",
        "  def inception_module(self, x, reduce=16, expand=64):\n",
        "    # 1x1\n",
        "    conv1x1 = self.basic_conv2d(x, expand, (1,1), (1,1), 'valid')\n",
        "    # 3x3\n",
        "    conv3x3_reduce = self.basic_conv2d(x, reduce, (1,1), (1,1), 'valid')\n",
        "    conv3x3 = self.basic_conv2d(conv3x3_reduce, expand, (3,3), (1,1), 'same')\n",
        "    # 5x5\n",
        "    conv5x5_reduce = self.basic_conv2d(x, reduce, (1,1), (1,1), 'valid')\n",
        "    conv5x5 = self.basic_conv2d(conv5x5_reduce, expand, (5,5), (1,1), 'same')\n",
        "    # プーリング層\n",
        "    max_pool = MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
        "    pool_proj = self.basic_conv2d(max_pool, expand, (1,1), (1,1), 'valid')\n",
        "    # 結合層\n",
        "    x = concatenate([conv1x1, conv3x3, conv5x5, pool_proj], axis=3)\n",
        "    return x\n",
        "  \n",
        "  # GoogleNet(LesNetのパロディ)\n",
        "  def google_net(self):\n",
        "    input=Input(shape=(200,200,4))\n",
        "    x = self.basic_conv2d(input, 64, (7,7), (2,2), 'same')\n",
        "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='valid')(x)\n",
        "    x = self.basic_conv2d(x, 64, (1,1), (1,1), 'valid')\n",
        "    x = self.basic_conv2d(x, 192, (3,3), (1,1), 'same')\n",
        "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='valid')(x)\n",
        "    x = self.inception_module(x, 16, 16)\n",
        "    x = self.inception_module(x, 16, 16)\n",
        "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='valid')(x)\n",
        "    x = self.inception_module(x, 32, 32)\n",
        "    x = self.inception_module(x, 32, 32)\n",
        "    x = self.inception_module(x, 32, 32)\n",
        "    x = self.inception_module(x, 32, 32)\n",
        "    x = self.inception_module(x, 32, 32)\n",
        "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='valid')(x)\n",
        "    x = self.inception_module(x, 64, 64)\n",
        "    x = self.inception_module(x, 64, 64)\n",
        "    x = AveragePooling2D(pool_size=(1,1))(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dropout(0.7)(x)\n",
        "    x = Dense(1000, activation=\"linear\")(x)\n",
        "    model = Model(inputs = input, outputs = x)\n",
        "    model.summary()\n",
        "    # ～ここまでGoogleNet\n",
        "    im_in1 = Input(shape=(200,200,4))\n",
        "    im_in2 = Input(shape=(200,200,4))\n",
        "    feat_x1 = model(im_in1)\n",
        "    feat_x2 = model(im_in2)\n",
        "    lambda_merge = Lambda(self.euclidean_distance)([feat_x1, feat_x2])\n",
        "    model_final = Model(inputs = [im_in1, im_in2], outputs = lambda_merge)\n",
        "    model_final.summary()\n",
        "    # ～ここまで顔の類似度チェック(ユークリッド距離)\n",
        "    adam = Adam(learning_rate=0.001)\n",
        "    # 最適化関数：adam, 損失関数：contrastive loss\n",
        "    model_final.compile(optimizer=adam, loss=self.contrastive_loss, metrics=['accuracy'])\n",
        "    return model_final\n",
        "  \n",
        "  # 学習率の可視化と\n",
        "  def show_graph(self):\n",
        "    gen = self.generator(32)\n",
        "    val_gen = self.val_generator(8)\n",
        "    model_final = self.google_net()\n",
        "    history = model_final.fit(gen, steps_per_epoch=30, epochs=50, validation_data = val_gen, validation_steps=20)\n",
        "    cop = self.create_wrong_rgbd('val')\n",
        "    outputs = model_final.predict([cop[0].reshape((1,200,200,4)), cop[1].reshape((1,200,200,4))])\n",
        "    print(outputs)\n",
        "\n",
        "    # Plot training & validation accuracy values\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('Model accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "    # Plot training & validation loss values\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "mHRCA2V7FLi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f11 = faceid_v1_1()\n",
        "x = f11.show_graph()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8R0pd5SC518R",
        "outputId": "37cb08fb-3df1-4e9f-9016-24be9854d62b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_7 (InputLayer)           [(None, 200, 200, 4  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d_114 (Conv2D)            (None, 98, 98, 64)   6464        ['input_7[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_114 (Batch  (None, 98, 98, 64)  256         ['conv2d_114[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_114 (Activation)    (None, 98, 98, 64)   0           ['batch_normalization_114[0][0]']\n",
            "                                                                                                  \n",
            " max_pooling2d_26 (MaxPooling2D  (None, 48, 48, 64)  0           ['activation_114[0][0]']         \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_115 (Conv2D)            (None, 48, 48, 16)   1040        ['max_pooling2d_26[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_116 (Conv2D)            (None, 48, 48, 16)   272         ['conv2d_115[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_117 (Conv2D)            (None, 48, 48, 16)   2320        ['conv2d_115[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate_18 (Concatenate)   (None, 48, 48, 32)   0           ['conv2d_116[0][0]',             \n",
            "                                                                  'conv2d_117[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_118 (Conv2D)            (None, 48, 48, 16)   528         ['concatenate_18[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_119 (Conv2D)            (None, 48, 48, 16)   272         ['conv2d_118[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_120 (Conv2D)            (None, 48, 48, 16)   2320        ['conv2d_118[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate_19 (Concatenate)   (None, 48, 48, 32)   0           ['conv2d_119[0][0]',             \n",
            "                                                                  'conv2d_120[0][0]']             \n",
            "                                                                                                  \n",
            " max_pooling2d_27 (MaxPooling2D  (None, 23, 23, 32)  0           ['concatenate_19[0][0]']         \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_121 (Conv2D)            (None, 23, 23, 32)   1056        ['max_pooling2d_27[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_122 (Conv2D)            (None, 23, 23, 32)   1056        ['conv2d_121[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_123 (Conv2D)            (None, 23, 23, 32)   9248        ['conv2d_121[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate_20 (Concatenate)   (None, 23, 23, 64)   0           ['conv2d_122[0][0]',             \n",
            "                                                                  'conv2d_123[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_124 (Conv2D)            (None, 23, 23, 32)   2080        ['concatenate_20[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_125 (Conv2D)            (None, 23, 23, 32)   1056        ['conv2d_124[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_126 (Conv2D)            (None, 23, 23, 32)   9248        ['conv2d_124[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate_21 (Concatenate)   (None, 23, 23, 64)   0           ['conv2d_125[0][0]',             \n",
            "                                                                  'conv2d_126[0][0]']             \n",
            "                                                                                                  \n",
            " max_pooling2d_28 (MaxPooling2D  (None, 11, 11, 64)  0           ['concatenate_21[0][0]']         \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_127 (Conv2D)            (None, 11, 11, 48)   3120        ['max_pooling2d_28[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_128 (Conv2D)            (None, 11, 11, 48)   2352        ['conv2d_127[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_129 (Conv2D)            (None, 11, 11, 48)   20784       ['conv2d_127[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate_22 (Concatenate)   (None, 11, 11, 96)   0           ['conv2d_128[0][0]',             \n",
            "                                                                  'conv2d_129[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_130 (Conv2D)            (None, 11, 11, 48)   4656        ['concatenate_22[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_131 (Conv2D)            (None, 11, 11, 48)   2352        ['conv2d_130[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_132 (Conv2D)            (None, 11, 11, 48)   20784       ['conv2d_130[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate_23 (Concatenate)   (None, 11, 11, 96)   0           ['conv2d_131[0][0]',             \n",
            "                                                                  'conv2d_132[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_133 (Conv2D)            (None, 11, 11, 64)   6208        ['concatenate_23[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_134 (Conv2D)            (None, 11, 11, 64)   4160        ['conv2d_133[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_135 (Conv2D)            (None, 11, 11, 64)   36928       ['conv2d_133[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate_24 (Concatenate)   (None, 11, 11, 128)  0           ['conv2d_134[0][0]',             \n",
            "                                                                  'conv2d_135[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_136 (Conv2D)            (None, 11, 11, 64)   8256        ['concatenate_24[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_137 (Conv2D)            (None, 11, 11, 64)   4160        ['conv2d_136[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_138 (Conv2D)            (None, 11, 11, 64)   36928       ['conv2d_136[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate_25 (Concatenate)   (None, 11, 11, 128)  0           ['conv2d_137[0][0]',             \n",
            "                                                                  'conv2d_138[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 11, 11, 128)  0           ['concatenate_25[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_139 (Conv2D)            (None, 11, 11, 512)  66048       ['dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            " activation_115 (Activation)    (None, 11, 11, 512)  0           ['conv2d_139[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 253,952\n",
            "Trainable params: 253,824\n",
            "Non-trainable params: 128\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_8 (InputLayer)        [(None, 200, 200, 4)]     0         \n",
            "                                                                 \n",
            " model_4 (Functional)        (None, 11, 11, 512)       253952    \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 61952)             0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 512)               31719936  \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " batch_normalization_115 (Ba  (None, 512)              2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 128)               65664     \n",
            "                                                                 \n",
            " lambda_2 (Lambda)           (None, 128)               0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 32,041,600\n",
            "Trainable params: 32,040,448\n",
            "Non-trainable params: 1,152\n",
            "_________________________________________________________________\n",
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_9 (InputLayer)           [(None, 200, 200, 4  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " input_10 (InputLayer)          [(None, 200, 200, 4  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " model_5 (Functional)           (None, 128)          32041600    ['input_9[0][0]',                \n",
            "                                                                  'input_10[0][0]']               \n",
            "                                                                                                  \n",
            " lambda_3 (Lambda)              (None, 1)            0           ['model_5[0][0]',                \n",
            "                                                                  'model_5[1][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 32,041,600\n",
            "Trainable params: 32,040,448\n",
            "Non-trainable params: 1,152\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "30/30 [==============================] - 39s 1s/step - loss: 0.2582 - accuracy: 0.7312 - val_loss: 0.2936 - val_accuracy: 0.5125\n",
            "Epoch 2/50\n",
            "30/30 [==============================] - 35s 1s/step - loss: 0.0970 - accuracy: 0.9198 - val_loss: 0.3103 - val_accuracy: 0.5063\n",
            "Epoch 3/50\n",
            "30/30 [==============================] - 36s 1s/step - loss: 0.0733 - accuracy: 0.9438 - val_loss: 0.2581 - val_accuracy: 0.5437\n",
            "Epoch 4/50\n",
            "30/30 [==============================] - 35s 1s/step - loss: 0.0688 - accuracy: 0.9490 - val_loss: 0.1026 - val_accuracy: 0.8750\n",
            "Epoch 5/50\n",
            "30/30 [==============================] - 37s 1s/step - loss: 0.0678 - accuracy: 0.9531 - val_loss: 0.1111 - val_accuracy: 0.8500\n",
            "Epoch 6/50\n",
            "30/30 [==============================] - 35s 1s/step - loss: 0.0588 - accuracy: 0.9635 - val_loss: 0.0708 - val_accuracy: 0.9000\n",
            "Epoch 7/50\n",
            "30/30 [==============================] - 35s 1s/step - loss: 0.0581 - accuracy: 0.9531 - val_loss: 0.0927 - val_accuracy: 0.8687\n",
            "Epoch 8/50\n",
            "30/30 [==============================] - 35s 1s/step - loss: 0.0560 - accuracy: 0.9615 - val_loss: 0.0811 - val_accuracy: 0.8875\n",
            "Epoch 9/50\n",
            "30/30 [==============================] - 35s 1s/step - loss: 0.0584 - accuracy: 0.9635 - val_loss: 0.0734 - val_accuracy: 0.8813\n",
            "Epoch 10/50\n",
            "30/30 [==============================] - 35s 1s/step - loss: 0.0501 - accuracy: 0.9698 - val_loss: 0.0791 - val_accuracy: 0.9000\n",
            "Epoch 11/50\n",
            "30/30 [==============================] - 35s 1s/step - loss: 0.0493 - accuracy: 0.9625 - val_loss: 0.1677 - val_accuracy: 0.7312\n",
            "Epoch 12/50\n",
            "30/30 [==============================] - 37s 1s/step - loss: 0.0541 - accuracy: 0.9635 - val_loss: 0.1199 - val_accuracy: 0.8375\n",
            "Epoch 13/50\n",
            "30/30 [==============================] - 36s 1s/step - loss: 0.0459 - accuracy: 0.9677 - val_loss: 0.0907 - val_accuracy: 0.8687\n",
            "Epoch 14/50\n",
            "30/30 [==============================] - 35s 1s/step - loss: 0.0420 - accuracy: 0.9771 - val_loss: 0.0786 - val_accuracy: 0.8750\n",
            "Epoch 15/50\n",
            "30/30 [==============================] - 35s 1s/step - loss: 0.0439 - accuracy: 0.9729 - val_loss: 0.0655 - val_accuracy: 0.9000\n",
            "Epoch 16/50\n",
            "30/30 [==============================] - 35s 1s/step - loss: 0.0432 - accuracy: 0.9802 - val_loss: 0.0605 - val_accuracy: 0.9125\n",
            "Epoch 17/50\n",
            "30/30 [==============================] - 35s 1s/step - loss: 0.0369 - accuracy: 0.9740 - val_loss: 0.0596 - val_accuracy: 0.8875\n",
            "Epoch 18/50\n",
            "30/30 [==============================] - 35s 1s/step - loss: 0.0421 - accuracy: 0.9729 - val_loss: 0.0596 - val_accuracy: 0.9125\n",
            "Epoch 19/50\n",
            "30/30 [==============================] - 37s 1s/step - loss: 0.0379 - accuracy: 0.9792 - val_loss: 0.0488 - val_accuracy: 0.9062\n",
            "Epoch 20/50\n",
            "30/30 [==============================] - 35s 1s/step - loss: 0.0349 - accuracy: 0.9812 - val_loss: 0.0586 - val_accuracy: 0.9125\n",
            "Epoch 21/50\n",
            "30/30 [==============================] - 35s 1s/step - loss: 0.0374 - accuracy: 0.9802 - val_loss: 0.0808 - val_accuracy: 0.8438\n",
            "Epoch 22/50\n",
            "30/30 [==============================] - 35s 1s/step - loss: 0.0396 - accuracy: 0.9781 - val_loss: 0.0558 - val_accuracy: 0.9187\n",
            "Epoch 23/50\n",
            "30/30 [==============================] - 35s 1s/step - loss: 0.0374 - accuracy: 0.9781 - val_loss: 0.0555 - val_accuracy: 0.9187\n",
            "Epoch 24/50\n",
            "30/30 [==============================] - 35s 1s/step - loss: 0.0399 - accuracy: 0.9688 - val_loss: 0.0395 - val_accuracy: 0.9375\n",
            "Epoch 25/50\n",
            "30/30 [==============================] - 35s 1s/step - loss: 0.0359 - accuracy: 0.9792 - val_loss: 0.0479 - val_accuracy: 0.9375\n",
            "Epoch 26/50\n",
            "30/30 [==============================] - 36s 1s/step - loss: 0.0409 - accuracy: 0.9740 - val_loss: 0.0796 - val_accuracy: 0.8500\n",
            "Epoch 27/50\n",
            "30/30 [==============================] - 36s 1s/step - loss: 0.0379 - accuracy: 0.9771 - val_loss: 0.0873 - val_accuracy: 0.8750\n",
            "Epoch 28/50\n",
            "30/30 [==============================] - 35s 1s/step - loss: 0.0375 - accuracy: 0.9812 - val_loss: 0.0772 - val_accuracy: 0.8875\n",
            "Epoch 29/50\n",
            "30/30 [==============================] - 35s 1s/step - loss: 0.0354 - accuracy: 0.9792 - val_loss: 0.0658 - val_accuracy: 0.9000\n",
            "Epoch 30/50\n",
            "30/30 [==============================] - 35s 1s/step - loss: 0.0337 - accuracy: 0.9792 - val_loss: 0.0553 - val_accuracy: 0.9062\n",
            "Epoch 31/50\n",
            "30/30 [==============================] - 35s 1s/step - loss: 0.0395 - accuracy: 0.9771 - val_loss: 0.0990 - val_accuracy: 0.8438\n",
            "Epoch 32/50\n",
            "30/30 [==============================] - 35s 1s/step - loss: 0.0324 - accuracy: 0.9844 - val_loss: 0.0465 - val_accuracy: 0.9312\n",
            "Epoch 33/50\n",
            "30/30 [==============================] - 35s 1s/step - loss: 0.0368 - accuracy: 0.9750 - val_loss: 0.0467 - val_accuracy: 0.9312\n",
            "Epoch 34/50\n",
            "30/30 [==============================] - 37s 1s/step - loss: 0.0320 - accuracy: 0.9823 - val_loss: 0.0681 - val_accuracy: 0.8750\n",
            "Epoch 35/50\n",
            "30/30 [==============================] - 35s 1s/step - loss: 0.0384 - accuracy: 0.9656 - val_loss: 0.0522 - val_accuracy: 0.9250\n",
            "Epoch 36/50\n",
            "30/30 [==============================] - 35s 1s/step - loss: 0.0369 - accuracy: 0.9740 - val_loss: 0.0890 - val_accuracy: 0.8562\n",
            "Epoch 37/50\n",
            "30/30 [==============================] - 35s 1s/step - loss: 0.0406 - accuracy: 0.9740 - val_loss: 0.0559 - val_accuracy: 0.9000\n",
            "Epoch 38/50\n",
            "30/30 [==============================] - 35s 1s/step - loss: 0.0375 - accuracy: 0.9771 - val_loss: 0.0639 - val_accuracy: 0.9062\n",
            "Epoch 39/50\n",
            "30/30 [==============================] - 35s 1s/step - loss: 0.0346 - accuracy: 0.9719 - val_loss: 0.0477 - val_accuracy: 0.9312\n",
            "Epoch 40/50\n",
            "30/30 [==============================] - 35s 1s/step - loss: 0.0310 - accuracy: 0.9802 - val_loss: 0.0515 - val_accuracy: 0.9187\n",
            "Epoch 41/50\n",
            "22/30 [=====================>........] - ETA: 8s - loss: 0.0326 - accuracy: 0.9844"
          ]
        }
      ]
    }
  ]
}